{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "9m7NnKO98fBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "6YO_19v9gE9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sastrawi\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
      ],
      "metadata": {
        "id": "ACgKyB5OgKuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Production"
      ],
      "metadata": {
        "id": "tUJXY3q-uB-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "  # case folding\n",
        "  text = text.lower()\n",
        "  # number removal\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "  # punctuation removal\n",
        "  text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "  # extra \\n and \\s removal\n",
        "  text = re.sub(r'\\s+', ' ', text.replace('\\n', ' ')).strip()\n",
        "  # stopword removal\n",
        "  stopword_remover = StopWordRemoverFactory().create_stop_word_remover()\n",
        "  text = stopword_remover.remove(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "OL0d476uuQNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_dim):\n",
        "  model = model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(32, input_dim=input_dim, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(29, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "XvIU6kvguZQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset into pandas dataframe\n",
        "df = pd.read_csv(\"/content/data.csv\")\n",
        "# drop rows with missing and duplicate value\n",
        "df_clean = df.dropna().drop_duplicates()"
      ],
      "metadata": {
        "id": "02c2phoJvH9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare feature (X) and target (y) variable for model training & validation purpose\n",
        "# the feature that will be used is a preprocessed form of the raw article content\n",
        "X = pd.Series(df_clean['article_content'].map(lambda x: normalize_text(x)))\n",
        "y = df_clean['article_topic']"
      ],
      "metadata": {
        "id": "Wxx0rpYZyNzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the original dataset will be divided based on article topic distribution (stratified sampling)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df_clean['article_topic'])"
      ],
      "metadata": {
        "id": "M71y711svmZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize train & validation set so that it can be further processed/learned by the ML model\n",
        "feature_vectorizer = TfidfVectorizer(lowercase=False)\n",
        "vectorizer = feature_vectorizer.fit(X_train)\n",
        "vectorized_X_train = feature_vectorizer.transform(X_train).toarray()\n",
        "vectorized_X_val = feature_vectorizer.transform(X_val).toarray()\n",
        "# tfidfvectorizer produces a sparse array and this is not good for keras, hence we use toarray()\n",
        "target_vectorizer = LabelBinarizer()\n",
        "target_vectorizer.fit(y_train)\n",
        "vectorized_y_train = target_vectorizer.transform(y_train)\n",
        "vectorized_y_val = target_vectorizer.transform(y_val)"
      ],
      "metadata": {
        "id": "A6nC57J3vnmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# develop the model\n",
        "model = build_model(vectorized_X_train.shape[1])"
      ],
      "metadata": {
        "id": "y9iVGzzFvq5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "training_history = model.fit(vectorized_X_train, vectorized_y_train,\n",
        "                             epochs=100,\n",
        "                             verbose=1,\n",
        "                             validation_data=(vectorized_X_val, vectorized_y_val),\n",
        "                             batch_size=32,\n",
        "                             callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode=\"min\", patience=5, verbose=1)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByPEaCAvvsQl",
        "outputId": "bbdc4777-ab7f-461a-c2d7-66969c26be73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 20s 76ms/step - loss: 2.3866 - accuracy: 0.4753 - val_loss: 1.5803 - val_accuracy: 0.6157\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 1.3743 - accuracy: 0.6590 - val_loss: 1.0914 - val_accuracy: 0.7536\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 1.0146 - accuracy: 0.7516 - val_loss: 0.8588 - val_accuracy: 0.7938\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.8043 - accuracy: 0.7954 - val_loss: 0.7289 - val_accuracy: 0.8058\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 11s 45ms/step - loss: 0.6668 - accuracy: 0.8198 - val_loss: 0.6466 - val_accuracy: 0.8204\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 12s 49ms/step - loss: 0.5550 - accuracy: 0.8501 - val_loss: 0.5871 - val_accuracy: 0.8304\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.4802 - accuracy: 0.8653 - val_loss: 0.5425 - val_accuracy: 0.8440\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 11s 45ms/step - loss: 0.4134 - accuracy: 0.8837 - val_loss: 0.5061 - val_accuracy: 0.8570\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.3523 - accuracy: 0.9083 - val_loss: 0.4777 - val_accuracy: 0.8655\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 19s 77ms/step - loss: 0.3150 - accuracy: 0.9186 - val_loss: 0.4604 - val_accuracy: 0.8690\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.2717 - accuracy: 0.9336 - val_loss: 0.4441 - val_accuracy: 0.8705\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 12s 49ms/step - loss: 0.2448 - accuracy: 0.9387 - val_loss: 0.4398 - val_accuracy: 0.8726\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.2127 - accuracy: 0.9478 - val_loss: 0.4306 - val_accuracy: 0.8721\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 12s 47ms/step - loss: 0.1954 - accuracy: 0.9528 - val_loss: 0.4268 - val_accuracy: 0.8736\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.1733 - accuracy: 0.9597 - val_loss: 0.4292 - val_accuracy: 0.8746\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1678 - accuracy: 0.9581 - val_loss: 0.4314 - val_accuracy: 0.8751\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.1532 - accuracy: 0.9614 - val_loss: 0.4324 - val_accuracy: 0.8776\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.1371 - accuracy: 0.9673 - val_loss: 0.4350 - val_accuracy: 0.8771\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.1313 - accuracy: 0.9664 - val_loss: 0.4399 - val_accuracy: 0.8791\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.1245 - accuracy: 0.9699 - val_loss: 0.4500 - val_accuracy: 0.8781\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 12s 48ms/step - loss: 0.1110 - accuracy: 0.9718 - val_loss: 0.4630 - val_accuracy: 0.8771\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 19s 78ms/step - loss: 0.1133 - accuracy: 0.9690 - val_loss: 0.4682 - val_accuracy: 0.8771\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 17s 67ms/step - loss: 0.1022 - accuracy: 0.9739 - val_loss: 0.4724 - val_accuracy: 0.8776\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0982 - accuracy: 0.9747 - val_loss: 0.4901 - val_accuracy: 0.8781\n",
            "Epoch 24: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = '''\n",
        "Menteri Koordinator Bidang Perekonomian, Airlangga Hartarto membeberkan strategi ASEAN untuk menjadi mesin pertumbuhan ekonomi global.\n",
        "Airlangga menilai, kondisi ekonomi global saat ini sangat dinamis. Meski begitu, ASEAN berhasil pulih bahkan melampaui situasi pra-pandemi dengan total PDB USD 3,6 triliun di tahun 2022.\n",
        "Di sisi lain, Airlangga memproyeksi perekonomian global ke depan mengindikasikan pelemahan dan ketidakpastian pertumbuhan. Hal tersebut memberikan tantangan terhadap pertumbuhan ekonomi kawasan.\n",
        "Untuk itu, Airlangga bersama dengan Menteri Ekonomi se-ASEAN menyiapkan strategi jitu untuk menjadikan ASEAN sebagai mesin pertumbuhan ekonomi global\n",
        "'''"
      ],
      "metadata": {
        "id": "1OWuGtuu1YCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_input = normalize_text(input)\n",
        "vectorized_input = vectorizer.transform([processed_input]).toarray()\n",
        "predicted_label = np.argmax(model.predict(vectorized_input), axis=-1)[0]\n",
        "\n",
        "label_dict = dict(enumerate(target_vectorizer.classes_))\n",
        "label_dict[predicted_label]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "TW5OQeOVuJKb",
        "outputId": "271e3940-9f5f-4e1a-93f4-7028c5078105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 39ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ekonomi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model.py"
      ],
      "metadata": {
        "id": "LwOtpkm_8jbF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxMsnHccf5-6"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = None\n",
        "        self.label_dict = None\n",
        "        self.trained_model = None\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        # case folding\n",
        "        text = text.lower()\n",
        "        # number removal\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        # punctuation removal\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        # extra \\n and \\s removal\n",
        "        text = re.sub(r'\\s+', ' ', text.replace('\\n', ' ')).strip()\n",
        "        # stopword removal\n",
        "        stopword_remover = StopWordRemoverFactory().create_stop_word_remover()\n",
        "        text = stopword_remover.remove(text)\n",
        "        return text\n",
        "\n",
        "    def prepare_dataset(self, data_path):\n",
        "        # load the dataset into pandas dataframe\n",
        "        df = pd.read_csv(data_path)\n",
        "        # drop rows with missing and duplicate value\n",
        "        df_clean = df.dropna().drop_duplicates()\n",
        "\n",
        "        # prepare feature (X) and target (y) variable for model training & validation purpose\n",
        "        # the feature that will be used is a preprocessed form of the raw article content\n",
        "        X = pd.Series(df_clean['article_content'].map(lambda x: self.normalize_text(x)))\n",
        "        y = df_clean['article_topic']\n",
        "\n",
        "        # the original dataset will be divided based on article topic distribution (stratified sampling)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df_clean['article_topic'])\n",
        "\n",
        "        # vectorize train & validation set so that it can be further processed/learned by the ML model\n",
        "        feature_vectorizer = TfidfVectorizer(lowercase=False)\n",
        "        self.vectorizer = feature_vectorizer.fit(X_train)\n",
        "        vectorized_X_train = feature_vectorizer.transform(X_train).toarray()\n",
        "        vectorized_X_val = feature_vectorizer.transform(X_val).toarray()\n",
        "        # tfidfvectorizer produces a sparse array and this is not good for keras, hence we use toarray()\n",
        "        target_vectorizer = LabelBinarizer()\n",
        "        target_vectorizer.fit(y_train)\n",
        "        self.label_dict = dict(enumerate(target_vectorizer.classes_))\n",
        "        vectorized_y_train = target_vectorizer.transform(y_train)\n",
        "        vectorized_y_val = target_vectorizer.transform(y_val)\n",
        "\n",
        "        return vectorized_X_train, vectorized_X_val, vectorized_y_train, vectorized_y_val\n",
        "\n",
        "    def build_model(self, input_dim):\n",
        "        model = model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Dense(32, input_dim=input_dim, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dropout(0.5))\n",
        "        model.add(tf.keras.layers.Dense(29, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def train(self):\n",
        "        X_train, X_test, y_train, y_test = self.prepare_dataset(\"/content/data.csv\")\n",
        "        self.trained_model = self.build_model(X_train.shape[1])\n",
        "        training = self.trained_model.fit(X_train, y_train,\n",
        "                                          epochs=100,\n",
        "                                          verbose=0,\n",
        "                                          validation_data=(X_test, y_test),\n",
        "                                          batch_size=32,\n",
        "                                          callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode=\"min\", patience=5, verbose=0)]\n",
        "                                          )\n",
        "\n",
        "    def predict(self, input):\n",
        "        processed_input = self.vectorizer.transform([self.normalize_text(input)]).toarray()\n",
        "        predicted_label = np.argmax(self.trained_model.predict(processed_input, verbose=0), axis=-1)[0]\n",
        "        return self.label_dict[predicted_label]\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\"\n",
        "        Save trained model to model.pickle file.\n",
        "        \"\"\"\n",
        "        ds.model.save(self, \"model.pickle\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # NOTE: Edit this if you add more initialization parameter\n",
        "    model = Model()\n",
        "\n",
        "    # # Train your model\n",
        "    # model.train()\n",
        "\n",
        "    # # Save your trained model to model.pickle\n",
        "    # model.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = Model()"
      ],
      "metadata": {
        "id": "GMRMRnrusQzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.train()"
      ],
      "metadata": {
        "id": "oxAgAIKIZOFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_1 = '''\n",
        "Menteri Koordinator Bidang Perekonomian, Airlangga Hartarto membeberkan strategi ASEAN untuk menjadi mesin pertumbuhan ekonomi global.\n",
        "Airlangga menilai, kondisi ekonomi global saat ini sangat dinamis. Meski begitu, ASEAN berhasil pulih bahkan melampaui situasi pra-pandemi dengan total PDB USD 3,6 triliun di tahun 2022.\n",
        "Di sisi lain, Airlangga memproyeksi perekonomian global ke depan mengindikasikan pelemahan dan ketidakpastian pertumbuhan. Hal tersebut memberikan tantangan terhadap pertumbuhan ekonomi kawasan.\n",
        "Untuk itu, Airlangga bersama dengan Menteri Ekonomi se-ASEAN menyiapkan strategi jitu untuk menjadikan ASEAN sebagai mesin pertumbuhan ekonomi global\n",
        "'''"
      ],
      "metadata": {
        "id": "LiLhWs99BsYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.predict(temp_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_TOgnla0Bupt",
        "outputId": "f1af4622-a02d-4e43-a06c-1303e96e5437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ekonomi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_2 = '''\n",
        "IMEI, atau International Mobile Equipment Identity, adalah nomor unik yang diberikan kepada setiap perangkat ponsel. Nomor ini memberikan identitas yang unik bagi setiap ponsel yang ada di dunia. Pastinya, ada fungsi IMEI yang perlu diketahui oleh para pengguna.\n",
        "Menurut buku Tips Ampuh Android, Tri Amperianto (2014:158), pada umumnya, IMEI berjumlah 15 digit atau lebih. Setiap perangkat akan mempunyai nomor IMEI tidak sama.\n",
        "Pengguna dapat memeriksa IMEI pada bagian belakang perangkat, atau biasanya tertempel pada sticker yang berada di bagian belakang boks tersebut.\n",
        "'''"
      ],
      "metadata": {
        "id": "psXuFpwhaE5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.predict(temp_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3941T3WNaWMT",
        "outputId": "a1c03443-007f-4903-bfd9-7ca118156d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Teknologi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_3 = '''\n",
        "Manchester City memetik kemenangan saat bertemu Fulham di laga keempat Liga Inggris. Bermain di Etihad Stadium, Sabtu (2/9) malam WIB, Man City menang dengan skor 4-1.\n",
        "Kemenangan Man City dinodai dengan gol yang dibuat Nathan Ake di injury time babak pertama. Sundulan Ake mengarah ke Manuel Akanji yang berada dalam posisi offside. Namun, Akanji meloloskan bola tanpa menyentuhnya hingga bola masuk ke gawang.\n",
        "Striker Man City, Erling Haaland, mengatakan wasit harusnya menganulir gol tersebut. Haaland bahkan amat merasakan kekesalan yang dialami pemain Fulham.\n",
        "'''"
      ],
      "metadata": {
        "id": "auon1CYFaXgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.predict(temp_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pYL8BiIjajCi",
        "outputId": "52651606-f909-4713-de32-3b6fd14608ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sepak Bola'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the model"
      ],
      "metadata": {
        "id": "6OSqpl71a2Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "uxZv_t8Ba39P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename_to_be_saved = 'model.pickle'"
      ],
      "metadata": {
        "id": "54Eu6okQa4z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(new_model, open(filename_to_be_saved, 'wb'))"
      ],
      "metadata": {
        "id": "akBTDXccbORT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickled_model = pickle.load(open('model.pickle', 'rb'))"
      ],
      "metadata": {
        "id": "WOhuzG1mn2Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickled_model.predict(temp_3)"
      ],
      "metadata": {
        "id": "xA5ddNwhn_3V",
        "outputId": "c83f3f79-0d95-422a-a92f-0db5875fa053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sepak Bola'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}